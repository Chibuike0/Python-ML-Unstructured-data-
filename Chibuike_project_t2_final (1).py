# -*- coding: utf-8 -*-
"""AML2203_sentiment analysis project_T2-FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-vvIoMeaa5DB4XwGOfFZcTu5YY6mU2Q

IMBD Movie critic analysis by Kyrian Okoroama.
The goal is to use the postive and negative reviews to train a ML algorithm to learn when a review is positive or negative thereby understanding the sentinment of the reviewer.

To achieve this, I had to understand how to handle unstructured data, python libraries to use, manage data imbalance, visualize the most frequent words, then prepare the data for ML model building.
"""

## load the csv datset into python envrioment
# Load the libraries for importing csv text file into python IDE without EOF errors
import os
import csv

# here we import the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# load NLTK library
import nltk
# Download the NLTK stopwords data
nltk.download('stopwords')
# Once downloaded, you can proceed with your original code
from nltk.corpus import stopwords

# load library for removing special character
import re

# load libraries for bagging the 30 most frequent words into vector form
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import nltk
nltk.download('punkt')  # Download the NLTK punkt tokenizer

## load library to perform TD-IDF using sklearn
from sklearn.feature_extraction.text import TfidfVectorizer

# load library for visualize keywords
from wordcloud import WordCloud

# load ploty for n-gram words plot
import plotly.express as px

"""## DATASET

DATASET: we select IMDB dataset because of the data size, complext text and it will be a good hands-on project.

The IMDB dataset comprises 50,000 movie reviews, making it suitable for natural language processing and text analytics. It serves as a dataset for binary sentiment classification and offers significantly more data compared to earlier benchmark datasets.

https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

Unfortunalteky we got error while loading the data which we try to solve with quote & error_bad_lines=false.

**IF YOU GET EOF ERROR! KINDLY DELETE PATH STRING ON COLAB, COPY THE PATH AGAIN AND PASTE. THIS LOADS THE DATA**
"""

# load the csv text datset into python envrioment
# due to error loading dataset using pandas DF to load the text file, OS libary was used instead and then used pandas to create a datframe.

# Define the file path
df_data_path = '/content/IMDB_Dataset-saved.csv'

# Check if the file exists
if os.path.exists(df_data_path):
    print("File exists.")

    # Load the CSV file into a DataFrame
    df_data = pd.read_csv(df_data_path, quoting=csv.QUOTE_ALL, error_bad_lines=False)

    # Display the first 5 rows of the DataFrame
    print(df_data.head(5))
else:
    print("File does not exist.")

# display the first 5 rows of dataset
df_data.head(5)

df_data.info() # check for the data type, all features are object

# No empty entities
df_data.isnull().sum()

"""Since there are no null values, we will check for data imbalance on Setiment colum which will be our target feature for modeling"""

df_data['sentiment'].value_counts() # counts number of times positve and negative sentiment were recorded.

"""Target feature is balanced"""

# plot sentiment feature to check for data imbalance
sns.countplot(data=df_data, x='sentiment')
plt.title('Check for data imbalance')
plt.show()

"""## DATA PREPROCESSING

Our goal is to assess the positivity or negativity of reviews based on the language used. We exclude stop words like "would," "shall," "could," and "might" since they don't contribute much to understanding the sentiment. These words usually revolve around possibilities or actions and don't strongly indicate whether the reviewers liked or disliked the movie.

Nevertheless, we make an exception for the word "not" in our analysis because it can significantly impact the sentiment of a sentence. For instance, while "We enjoyed the movie" is positive, the introduction of "not" in "We did not enjoy the movie" turns the sentiment negative.

Ultimately, all these procedures are consolidated into a data preprocessing function. This function takes a text input and systematically applies the below operations to cleanse and ready the text for subsequent analysis. The outcome is refined and well-structured text data suitable for various tasks, including sentiment analysis, text classification, or any other text-centric analyses.
"""

# remove not from stop words and new stop words to the stop words list

# Get the default English stopwords from NLTK
stop_words = set(stopwords.words('english'))

# Define a list of new stopwords to add to the default list
new_stopwords = ["would", "shall", "could", "might", "br"]

# Update the default stopwords set with the new stopwords
stop_words.update(new_stopwords)

# Remove the word "not" from the stopwords set (optional, depending on your analysis needs)
stop_words.discard("not")

# Print the final set of stopwords
print(stop_words)

"""## TEXT CLEANING

1. Special Character Removal: In this phase, we eliminate special characters such as punctuation marks, symbols, and other non-alphanumeric characters from the text. These elements typically do not significantly contribute to comprehending the meaning of the text, so we exclude them.
"""

#1. removing special characters
# Define a function to remove special characters
def remove_special_characters(text):
    # Using regular expression to remove special characters
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

# Apply the function to the 'review' column
df_data['review'] = df_data['review'].apply(remove_special_characters)

# Display the updated DataFrame
print(df_data)

"""2. URL Elimination: At this stage, we eradicate web links or URLs from the text. Since these links do not offer valuable information for our analysis, we opt to remove them.


"""

#2. removing URL and weblinks
# Define a function to remove URLs
def remove_urls(text):
    # Using regular expression to remove URLs
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

# Apply the function to the 'review' column
df_data['review'] = df_data['review'].apply(remove_urls)

# Display the updated DataFrame
print(df_data)

"""3. Stopword Removal and Elimination of Short Words: Stopwords, such as common words like "the," "and," "is," etc., which frequently appear in the English language but often lack essential meaning in the context of text analysis, are extracted from the text. Simultaneously, words with a length less than three are also excluded as they usually lack meaningful content.


"""

# Stop words removal from the review
import pandas as pd
from nltk.corpus import stopwords


# Download NLTK stopwords data
import nltk
nltk.download('stopwords')

# Get the set of English stopwords from NLTK
stop_words = set(stopwords.words('english'))

# Define a function to remove stopwords and short words
def preprocess_text(text):
    # Split the text into words
    words = text.split()

    # Remove stopwords and short words (length less than 3)
    filtered_words = [word for word in words if word.lower() not in stop_words and len(word) >= 3]

    # Join the filtered words back into a sentence
    return ' '.join(filtered_words)

# Apply the function to the 'review' column
df_data['review'] = df_data['review'].apply(preprocess_text)

# Display the updated DataFrame
print(df_data)

"""4. Contraction Expansion: Contractions, representing shortened forms of words like "don't" for "do not" or "won't" for "will not," are expanded back to their full forms in this phase. This ensures consistent treatment of words and a better understanding of their intended meanings.


"""

# Perform Contraction Expansion
# Define a contraction mapping
contraction_mapping = {
    "don't": "do not",
    "donot": "do not",
    "doesnot": "does not",
    "didn't": "did not",
    "didnt": "did not",
    "didnot": "did not",
    "won't": "would not",
    "wont": "would not",
    "wouldnt": "would not",
    "couldnt": "could not",
    "can't": "can not",
    "cannot": "can not",
    "doesn't": "does not",
    "shouldn't": "should not",
    "needn't": "need not",
    "hasn't": "has not",
    "haven't": "has not",
    "weren't": "has not",
    "mightn't": "might not",
    "wasn't": "was not",
    "wasnt": "was not",
    "n't": "not",
    "Ive": "i have",
}

# Define a function to expand contractions
def expand_contractions(text, contraction_mapping):
    for contraction, expanded_form in contraction_mapping.items():
        text = text.replace(contraction, expanded_form)
    return text

# Apply the function to the 'review' column
df_data['review'] = df_data['review'].apply(lambda x: expand_contractions(x, contraction_mapping))

# Display the updated DataFrame
print(df_data)

df_data.head() # head 5 rows of cleaned and processed  text

"""## EXPLORATORY TEXT ANALYSIS

First we visualize the keywords from the sentiment feature to see the most used positive & negative words using wordcloud
"""

# visualize the positive keywords

# Filter only positive sentiments
positive_reviews = df_data[df_data['sentiment'] == 'positive']

# Join all positive reviews into a single string
positive_text = ' '.join(positive_reviews['review'])

# Create a WordCloud instance
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)

# Display the WordCloud using Matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Positive Words')
plt.show()

# visualize the negative keywords

# Filter only negative sentiments
negative_reviews = df_data[df_data['sentiment'] == 'negative']

# Join all positive reviews into a single string
negative_text = ' '.join(negative_reviews['review'])

# Create a WordCloud instance
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_text)

# Display the WordCloud using Matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Negative Words')
plt.show()

"""Plot density of average words used in the sentiment because we want understand each reviewer lenght of words while being either positive or negative.

We coud see that reviewers use more words while being negative than positive
"""

# Create subplots for positive and negative sentiments
figure, (pos_ax, neg_ax) = plt.subplots(1, 2, figsize=(15, 8))

sentiments = ['positive', 'negative']
colors = ['green', 'red']

for ax, sentiment, color in zip([pos_ax, neg_ax], sentiments, colors):
    reviews = df_data[df_data['sentiment'] == sentiment]
    word_lengths = reviews['review'].str.split().apply(lambda x: [len(i) for i in x])

    # Use histplot for histogram and KDE
    sns.histplot(word_lengths.map(np.mean), ax=ax, color=color, kde=True, stat='density')

    ax.set_title(f'Average Word Length in {sentiment.capitalize()} Reviews')
    ax.set_xlabel('Average Word Length')
    ax.set_ylabel('Density')

figure.suptitle('Average Word Length in Reviews')
plt.show()

"""Furthermore, we explore the words used for reviews in both negative and positive. We will explore one, four, five words and understand any hidden meaning when the words are more than one.

POSITIVE reviews N-grams
"""

# Filter only positive sentiments
positive_reviews = df_data[df_data['sentiment'] == 'positive']

# Tokenize the positive reviews into individual words
positive_words = ' '.join(positive_reviews['review']).split()

# Create a DataFrame with the word frequencies
word_freq = pd.DataFrame({'word': positive_words})
word_freq = word_freq['word'].value_counts().reset_index()
word_freq.columns = ['word', 'count']

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 Unigrams in Positive Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer


# Filter only positive sentiments
positive_reviews = df_data[df_data['sentiment'] == 'positive']

# Tokenize the positive reviews into 4-grams using CountVectorizer
vectorizer = CountVectorizer(ngram_range=(4, 4))
X = vectorizer.fit_transform(positive_reviews['review'])
features = vectorizer.get_feature_names_out()

# Sum the counts of 4-grams
word_freq = pd.DataFrame({'word': features, 'count': X.sum(axis=0).tolist()[0]})

# Sort by count in descending order
word_freq = word_freq.sort_values(by='count', ascending=False)

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 4-grams in Positive Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer


# Filter only positive sentiments
positive_reviews = df_data[df_data['sentiment'] == 'positive']

# Tokenize the positive reviews into 4-grams using CountVectorizer
vectorizer = CountVectorizer(ngram_range=(5, 5))
X = vectorizer.fit_transform(positive_reviews['review'])
features = vectorizer.get_feature_names_out()

# Sum the counts of 5-grams
word_freq = pd.DataFrame({'word': features, 'count': X.sum(axis=0).tolist()[0]})

# Sort by count in descending order
word_freq = word_freq.sort_values(by='count', ascending=False)

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 5-grams in Positive Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

"""NEGATIVE reviews N-grams"""

# Filter only negative sentiments
negative_reviews = df_data[df_data['sentiment'] == 'negative']

# Tokenize the negative reviews into individual words
negative_words = ' '.join(negative_reviews['review']).split()

# Create a DataFrame with the word frequencies
word_freq = pd.DataFrame({'word': negative_words})
word_freq = word_freq['word'].value_counts().reset_index()
word_freq.columns = ['word', 'count']

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 Unigrams in negative Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer


# Filter only negative sentiments
negative_reviews = df_data[df_data['sentiment'] == 'negative']

# Tokenize the negative reviews into 4-grams using CountVectorizer
vectorizer = CountVectorizer(ngram_range=(4, 4))
X = vectorizer.fit_transform(negative_reviews['review'])
features = vectorizer.get_feature_names_out()

# Sum the counts of 4-grams
word_freq = pd.DataFrame({'word': features, 'count': X.sum(axis=0).tolist()[0]})

# Sort by count in descending order
word_freq = word_freq.sort_values(by='count', ascending=False)

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 4-grams in negative Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

import pandas as pd
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer


# Filter only negative sentiments
negative_reviews = df_data[df_data['sentiment'] == 'negative']

# Tokenize the negative reviews into 5-grams using CountVectorizer
vectorizer = CountVectorizer(ngram_range=(5, 5))
X = vectorizer.fit_transform(negative_reviews['review'])
features = vectorizer.get_feature_names_out()

# Sum the counts of 4-grams
word_freq = pd.DataFrame({'word': features, 'count': X.sum(axis=0).tolist()[0]})

# Sort by count in descending order
word_freq = word_freq.sort_values(by='count', ascending=False)

# Plot the bar chart using Plotly Express
fig = px.bar(word_freq.head(20), x='word', y='count')
fig.update_layout(title='Top 20 5-grams in negative Reviews', title_x=0.5)  # Set title_x to 0.5 for center alignment
fig.show()

"""For both positve and negative N-gram visuals, we mnoticed that there are common words used on both cases, therefore did not paint negative or positive picture. Therefore we opted for 4-gram and 5-gram becasue this will give more sense on words combinations.

So we could see that WORST ever for negative and BEST, GREATEST for positive reviews being used in 4 & 5 grams of word. THis painted a better picture of how the reviewers felt.

## MODELING

Before modeling we had to

A. We utilizes NLTK's WordNetLemmatizer to perform lemmatization on the words within the 'review' column of the DataFrame. The function lemmatize_text breaks down the text into individual words, lemmatizes each word, and subsequently combines them into a string. The resulting lemmatized reviews are saved in a newly created column named 'lemmatized_review' within the DataFrame.
"""

# lemmatization of word

import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download the WordNet resource if not already downloaded
import nltk
nltk.download('wordnet')

# Create a lemmatizer
lemmatizer = WordNetLemmatizer()

# Define a function for lemmatization
def lemmatize_text(text):
    tokens = word_tokenize(text)
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized_tokens)

# Apply lemmatization to the 'review' column
df_data['lemmatized_review'] = df_data['review'].apply(lemmatize_text)

# Display the DataFrame with lemmatized reviews
print(df_data[['review', 'lemmatized_review']])

"""B. bag the modes the most 30 frequent words into vector format. We can not bag all the words due to large data set.

Bagging takes the frequency of the most used words and assigns importance based on number of occurrence.
"""

# BAGGING THE WORDS

# Tokenize the text in the 'review' column
tokenized_reviews = [word_tokenize(review) for review in df_data['review']]

# Flatten the list of tokenized words
all_words = [word.lower() for sublist in tokenized_reviews for word in sublist]

# Create a frequency distribution of words
freq_dist = FreqDist(all_words)

# Get the 30 most frequent words
most_frequent_words = freq_dist.most_common(30)

# Extract the words from the tuples
most_frequent_words = [word[0] for word in most_frequent_words]

# Create a bag-of-words representation for the first 30 most frequent words
bag_of_words = pd.DataFrame({word: [1 if word in review else 0 for review in tokenized_reviews] for word in most_frequent_words})

# Display the resulting DataFrame with the bag-of-words representation
print(bag_of_words)

"""
C. we had use Term frequency-Inverser document frequency because TF checks the most frequent words and assign importance in a current document by dividing the frequency by total numner of words in a sentence. While IDF socre the rareness of the words across the document by using logarithm of number of sentences divided number of sentences containing in that word.

So we decided to use TD-IDF becasue it takes into consideration of all the words while BoW will give us compoutational issue or limit us to 30 most frequent words"""

## PERFORM TD-IDF using sklearn

import pandas as pd
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

# Tokenize the text in the 'review' column
tokenized_reviews = [' '.join(word_tokenize(review)) for review in df_data['review']]

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=30)  # Adjust max_features as needed

# Fit and transform the reviews
tfidf_matrix = vectorizer.fit_transform(tokenized_reviews)

# Create a DataFrame from the TF-IDF matrix
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Display the resulting DataFrame with the TF-IDF representation
print(tfidf_df)

"""Furthermore, we will build four models for accuracy comparison: they are Random Forest, Decison Tree, ADA Boost & Tensor flow and make co parison based on their accuracy, F1 score, AUC etc."""

# Model#1: Random forest model and Libraries for the ML
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Convert string labels to binary (0 or 1) for all algorithm
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_data['sentiment'])

# Assuming 'review' is the feature column
X = df_data['review']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed

# Fit and transform the training data
X_train_tfidf = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test_tfidf = vectorizer.transform(X_test)

# Build a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train_tfidf, y_train)

# Make predictions on the testing data
y_pred = rf_model.predict(X_test_tfidf)

# Calculate AUC
y_prob = rf_model.predict_proba(X_test_tfidf)[:, 1]
auc = roc_auc_score(y_test, y_prob)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Plot confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Print results
print(f"Accuracy: {accuracy}")
print(f"AUC: {auc}")
print("Classification Report:")
print(classification_rep)

#Model#2: Decision tree
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Build a Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model
dt_model.fit(X_train_tfidf, y_train)

# Make predictions on the testing data
y_pred = dt_model.predict(X_test_tfidf)

# Calculate AUC
y_prob = dt_model.predict_proba(X_test_tfidf)[:, 1]
auc = roc_auc_score(y_test, y_prob)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Plot confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Print results
print(f"Accuracy: {accuracy}")
print(f"AUC: {auc}")
print("Classification Report:")
print(classification_rep)

#Model#3: Ada BOOST
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Build an AdaBoost model
ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)

# Train the model
ada_model.fit(X_train_tfidf, y_train)

# Make predictions on the testing data
y_pred = ada_model.predict(X_test_tfidf)

# Calculate AUC
y_prob = ada_model.predict_proba(X_test_tfidf)[:, 1]
auc = roc_auc_score(y_test, y_prob)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='ADABoost')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

# Plot confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Print results
print(f"Accuracy: {accuracy}")
print(f"AUC: {auc}")
print("Classification Report:")
print(classification_rep)

# model#4 loading libraries for tensor flow model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Tensor flow model
# Convert string labels to binary (0 or 1)
label_encoder = LabelEncoder()
df_data['sentiment_encoded'] = label_encoder.fit_transform(df_data['sentiment'])
num_classes = len(label_encoder.classes_)

# Assuming 'review' is the feature column
X = df_data['review']
y = to_categorical(df_data['sentiment_encoded'], num_classes=num_classes)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize and pad sequences
max_words = 5000
max_len = 100
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')

# Build the model
embedding_dim = 50
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))
model.add(LSTM(units=100))
model.add(Dense(units=num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))

# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

"""ANALYSIS:

Random Forrest and Tensor flow performed better interms of accuracy, meaning the ability to analyze the text accurately was at 85%v with AUC of 93% while Tensor flow was model validation accurately after 5 epochs at 87% and train accuracy of 94%.

In this case, an AUC of 0.9311 is quite high, indicating that the Random Forest model performs well in distinguishing between positive and negative sentiment of our data.

In summary, the model demonstrates strong performance with an accuracy of 85.26%, and the AUC of 0.9311 suggests that the model has a high ability to discriminate between positive and negative sentiment however TensorFLow which is good with text data, had shown test accuracy of 87%. We are confident that Tensor flow performed better than Randowm forest

# Perfomring GridSearch CV Hyperparameter tuning on Forest model
"""

# importing the necessary libraries for the hyper parameter tuning
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import LabelEncoder

# Convert string labels to binary (0 or 1)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_data['sentiment'])

# Assuming 'review' is the feature column
X = df_data['review']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed

# Fit and transform the training data
X_train_tfidf = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test_tfidf = vectorizer.transform(X_test)

# GRID SEARCHCV hyperparameter model based on Random Forest model
rf_model = RandomForestClassifier()

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Use grid search to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train_tfidf, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train the model with the best hyperparameters
best_rf_model = RandomForestClassifier(**best_params)
best_rf_model.fit(X_train_tfidf, y_train)

# Make predictions on the testing data
y_pred = best_rf_model.predict(X_test_tfidf)

# Calculate AUC
y_prob = best_rf_model.predict_proba(X_test_tfidf)[:, 1]
auc = roc_auc_score(y_test, y_prob)

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# Print results
print(f"Best Hyperparameters: {best_params}")
print(f"Accuracy: {accuracy}")
print(f"AUC: {auc}")
print("Classification Report:")
print(classification_rep)

"""HYPERPARATER TUNING OF TENSORFLOW using Keras classifier because KerasClassifier is a wrapper class provided by Keras that allows us to use  Keras models as part of the scikit-learn library. This is useful when we want to leverage scikit-learn's tools for tasks such as hyperparameter tuning.

We will install several packages for TensorFlow
"""

# INstall tensorflow
#!pip uninstall tensorflow
#!pip install tensorflow==2.12.0
!pip install --upgrade tensorflow

!pip install scikeras # install scikeras library

!pip install tensorflow # install tensorflow library

!pip install keras # install keras library

pip install tensorflow==2.12.0 # due to error with wrapp scikit_learn, we install this

!pip install tensorflow keras scikit-learn # installig necesary library

# Import necessary libraries for TensorFlow Hyperparameter tuning

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Run the Tensorflow model

# Convert string labels to binary (0 or 1)
label_encoder = LabelEncoder()
df_data['sentiment_encoded'] = label_encoder.fit_transform(df_data['sentiment'])
num_classes = len(label_encoder.classes_)

# Assuming 'review' is the feature column
X = df_data['review']
y = to_categorical(df_data['sentiment_encoded'], num_classes=num_classes)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize and pad sequences
max_words = 5000
max_len = 100
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')

# Build the Sequential model
embedding_dim = 50
lstm_units = 100

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))
model.add(LSTM(units=lstm_units))
model.add(Dense(units=num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model with history for plotting
epochs = 5
batch_size = 32
history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))

# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

"""CONCLUSION: although it took over two hours for our hyperparameter to complete, we saw that there
That there was improvement on both Random Forrest and Tensorflow.

After several adjust and due to time and computing constrain, we could not adjust the model further. On this note we believ that model did well on modeling sentiment of the reviewers and can be used for further development.

However, we will prefer to say that TensorFlow will be the model we will further develop on because it perform better with text and performed slightly better than Random Forest.
"""